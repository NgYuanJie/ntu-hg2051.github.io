{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 09\n",
    "\n",
    "Overview\n",
    "\n",
    "* [**First-class Functions**](#First-class-Functions)\n",
    "* [**Higher-order Functions**](#Higher-order-Functions)\n",
    "* [**Recursive Functions**](#Recursive-Functions)\n",
    "* [**Bigrams**](#Bigrams)\n",
    "* [**N-grams**](#N-grams)\n",
    "* [**Collocations**](#Collocations)\n",
    "* [**Part-of-Speech Tags**](#Part-of-Speech-Tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "source": [
    "## First-class Functions\n",
    "\n",
    "Functions in Python are just a special kind of object, and you can use them in many ways that you can use other kinds of objects, like integers, strings, etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(x):\n",
    "    print(f'x={x}')\n",
    "\n",
    "func = function   # reassignment\n",
    "func('123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func == function  # comparison"
   ]
  },
  {
   "source": [
    "This flexibility enables higher-order functions.\n",
    "\n",
    "## Higher-order Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Use `filter()` with `str.isdigit()` to filter out non-numbers from a list of strings."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['2', '4', '6', '8']"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "strings = 'one 2 three 4 five 6 seven 8'.split()\n",
    "number_strings = list(filter(str.isdigit, strings))\n",
    "number_strings"
   ]
  },
  {
   "source": [
    "Now use `map()` with `int` to convert them all to integers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[2, 4, 6, 8]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "numbers = list(map(int, number_strings))\n",
    "numbers"
   ]
  },
  {
   "source": [
    "Write a higher-order function that takes a text-normalization function (such as `str.lower()`) and returns a new function that takes a list of strings and applies the normalization function to each one, returning a new list of strings."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"don't\", 'panic']"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "def make_normalizer(func):\n",
    "    def abc(strings):\n",
    "        return [func(s) for s in strings]\n",
    "    return abc\n",
    "\n",
    "all_upper = make_normalizer(str.upper)\n",
    "dont_panic = \"don't panic\".split()\n",
    "dont_panic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[\"DON'T\", 'PANIC']\n"
     ]
    }
   ],
   "source": [
    "print(all_upper(dont_panic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[\"Don'T\", 'Panic']\n"
     ]
    }
   ],
   "source": [
    "all_title = make_normalizer(str.title)\n",
    "print(all_title(dont_panic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"don't\", 'panic']"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "make_normalizer(str.lower)([\"DON'T\", \"Panic\"])"
   ]
  },
  {
   "source": [
    "## Recursive Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Write a recursive function to compute the fibonacci sequence, defined as:\n",
    "\n",
    "$$ fib(0) = 1 $$\n",
    "$$ fib(1) = 1 $$\n",
    "$$ fib(x) = fib(x-2) + fib(x-1) $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib(n):\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    elif n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return fib(n-2) + fib(n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14930352"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "fib(35)"
   ]
  },
  {
   "source": [
    "Try it out for a few values, like 3, 5, 10, 20, 40, ...\n",
    "\n",
    "Now try to write it iteratively."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterfib(n):\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    elif n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        a = b = 1\n",
    "        for _ in range(n - 1):\n",
    "            a, b = b, a+b\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "70330367711422815821835254877183549770181269836358732742604905087154537118196933579742249494562611733487750449241765991088186363265450223647106012053374121273867339111198139373125598767690091902245245323403501"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "iterfib(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams\n",
    "\n",
    "Bigrams are sequential pairs of items in a list. With language data, these items are generally words or characters. The bigrams (and n-grams in general) are like a sliding window of a small sequence of the data, and capture partial order information (e.g., \"the\" precedes \"dog\" in *the dog barked*). The bigrams for the sentence *Bigrams and n-grams are useful tools for computational linguists.* are as follows (with list indices indicated):\n",
    "\n",
    "```\n",
    "      Bigrams and n-grams are useful tools for computational linguists .\n",
    "     0       1   2       3   4      5     6   7             8         9 10\n",
    "0:2   Bigrams and\n",
    "1:3           and n-grams\n",
    "2:4               n-grams are\n",
    "3:5                       are useful\n",
    "4:6                           useful tools\n",
    "5:7                                  tools for\n",
    "6:8                                        for computational\n",
    "7:9                                            computational linguists\n",
    "8:10                                                         linguists .\n",
    "```\n",
    "\n",
    "**Task:** write a function to take an sequence and return a list of its bigrams. Compare your results with `nltk.bigrams()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on function ngrams in module nltk.util:\n\nngrams(sequence, n, pad_left=False, pad_right=False, left_pad_symbol=None, right_pad_symbol=None)\n    Return the ngrams generated from a sequence of items, as an iterator.\n    For example:\n    \n        >>> from nltk.util import ngrams\n        >>> list(ngrams([1,2,3,4,5], 3))\n        [(1, 2, 3), (2, 3, 4), (3, 4, 5)]\n    \n    Wrap with list for a list version of this function.  Set pad_left\n    or pad_right to true in order to get additional ngrams:\n    \n        >>> list(ngrams([1,2,3,4,5], 2, pad_right=True))\n        [(1, 2), (2, 3), (3, 4), (4, 5), (5, None)]\n        >>> list(ngrams([1,2,3,4,5], 2, pad_right=True, right_pad_symbol='</s>'))\n        [(1, 2), (2, 3), (3, 4), (4, 5), (5, '</s>')]\n        >>> list(ngrams([1,2,3,4,5], 2, pad_left=True, left_pad_symbol='<s>'))\n        [('<s>', 1), (1, 2), (2, 3), (3, 4), (4, 5)]\n        >>> list(ngrams([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n        [('<s>', 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, '</s>')]\n    \n    \n    :param sequence: the source data to be converted into ngrams\n    :type sequence: sequence or iter\n    :param n: the degree of the ngrams\n    :type n: int\n    :param pad_left: whether the ngrams should be left-padded\n    :type pad_left: bool\n    :param pad_right: whether the ngrams should be right-padded\n    :type pad_right: bool\n    :param left_pad_symbol: the symbol to use for left padding (default is None)\n    :type left_pad_symbol: any\n    :param right_pad_symbol: the symbol to use for right padding (default is None)\n    :type right_pad_symbol: any\n    :rtype: sequence or iter\n\n"
     ]
    }
   ],
   "source": [
    "help(nltk.ngrams)"
   ]
  },
  {
   "source": [
    "def bigrams(seq):\n",
    "    \"\"\"\n",
    "    Return the list of bigrams for *seq*.\n",
    "    \n",
    "    Each bigram on the list should be a tuple.\n",
    "    \"\"\"\n",
    "    return [tuple(seq[i:i+2]) for i in range(len(seq) - 1)]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('one', 'two'), ('two', 'three'), ('three', 'four')]"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "bigrams('one two three four'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test: [('Bigrams', 'and'), ('and', 'n-grams'), ('n-grams', 'are'), ('are', 'useful'), ('useful', 'tools'), ('tools', 'for'), ('for', 'computational'), ('computational', 'linguists'), ('linguists', '.')]\n\nNLTK: [('Bigrams', 'and'), ('and', 'n-grams'), ('n-grams', 'are'), ('are', 'useful'), ('useful', 'tools'), ('tools', 'for'), ('for', 'computational'), ('computational', 'linguists'), ('linguists', '.')]\n\nEqual? True\n"
     ]
    }
   ],
   "source": [
    "words = 'Bigrams and n-grams are useful tools for computational linguists .'.split()\n",
    "print('Test:', list(bigrams(words)))\n",
    "print()\n",
    "print('NLTK:', list(nltk.bigrams(words)))\n",
    "print()\n",
    "print('Equal?', list(bigrams(words)) == list(nltk.bigrams(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "Sometimes bigrams do not give enough context (they are not a big enough window into the sequence). N-grams are the general form of bigrams, where N=2. Write a function that takes a sequence and a number `n` and returns the list of n-grams for the sequence. Note that if `n` is greater than the length of the list, an empty list is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(seq, n):\n",
    "    \"\"\"\n",
    "    Return the list of n-grams for *seq* of length *n*.\n",
    "    \n",
    "    Each n-gram should be a tuple. If *n* is greater than the length\n",
    "    of *seq*, an empty list is returned. *n* must be greater than 0.\n",
    "    \"\"\"\n",
    "    return [tuple(seq[i:i+n]) for i in range(len(seq) - n + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1-grams: [('Bigrams',), ('and',), ('n-grams',), ('are',), ('useful',), ('tools',), ('for',), ('computational',), ('linguists',), ('.',)]\n\n2-grams: [('Bigrams', 'and'), ('and', 'n-grams'), ('n-grams', 'are'), ('are', 'useful'), ('useful', 'tools'), ('tools', 'for'), ('for', 'computational'), ('computational', 'linguists'), ('linguists', '.')]\n\n3-grams: [('Bigrams', 'and', 'n-grams'), ('and', 'n-grams', 'are'), ('n-grams', 'are', 'useful'), ('are', 'useful', 'tools'), ('useful', 'tools', 'for'), ('tools', 'for', 'computational'), ('for', 'computational', 'linguists'), ('computational', 'linguists', '.')]\n\n20-grams: []\n\nEqual? True\n"
     ]
    }
   ],
   "source": [
    "words = 'Bigrams and n-grams are useful tools for computational linguists .'.split()\n",
    "print('1-grams:', list(ngrams(words, 1)))\n",
    "print()\n",
    "print('2-grams:', list(ngrams(words, 2)))\n",
    "print()\n",
    "print('3-grams:', list(ngrams(words, 3)))\n",
    "print()\n",
    "print('20-grams:', list(ngrams(words, 20)))\n",
    "print()\n",
    "print('Equal?', list(ngrams(words, 3)) == list(nltk.ngrams(words, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations\n",
    "\n",
    "Note that some n-grams, such as `('are', 'useful')` are probably more frequent than others, such as `('Bigrams', 'and')`. N-grams whose components co-occur frequently are called \"collocations\". With an appropriate metric you can compute a collocation score for these, which could also be used as a threshold. Mutual information (MI) is a measure using the *observed frequencies* (O) and *expected frequencies* (E):\n",
    "\n",
    "$$ MI = log \\frac{O}{E} $$\n",
    "\n",
    "The observed frequency is how often we see the n-gram in the data. The expected frequency is how often we expect to see it given the frequencies of its components. Here is one way to define collocations (adapted from the reading):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# itemgetter() is a function that takes a number n and returns a\n",
    "# function which takes a sequence and returns the item at index n\n",
    "first = itemgetter(1)\n",
    "\n",
    "def collocations(words):\n",
    "    # Count the words and bigrams\n",
    "    wfd = nltk.FreqDist(words)\n",
    "    pfd = nltk.FreqDist(bigrams(words))\n",
    "\n",
    "    scored = [((w1,w2), score(w1, w2, wfd, pfd)) for w1, w2 in pfd]\n",
    "    ## sort according to the score\n",
    "    scored.sort(key=first, reverse=True)\n",
    "    return [p for (p,s) in scored]\n",
    "\n",
    "\n",
    "def score(word1, word2, wfd, pfd, power=3):\n",
    "    '''return the collocation score f(w1,w2)^power/(f(w1)*f(w2))'''\n",
    "    freq1 = wfd[word1]\n",
    "    freq2 = wfd[word2]\n",
    "    freq12 = pfd[(word1, word2)]\n",
    "    return freq12 ** power / float(freq1 * freq2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "firefox.txt ['does not', 'http ://', 'print preview', 'doesn work', 'drop down', 'dom inspector', 'download manager', 'context menu', 'address bar', 'right click', 'new tab', 'full screen', 'xml parsing', 'pwd mngr', 'tools options']\n",
      "grail.txt ['hello hello', 'black knight', 'saw saw', 'clop clop', 'pie iesu', 'iesu domine', 'mumble mumble', 'cart master', 'cartoon character', 'squeak squeak', 'burn her', 'round table', 'dramatic chord', 'run away', 'holy grail']\n",
      "overheard.txt ['new york', 'teen boy', 'teen girl', 'you know', 'middle aged', 'flight attendant', 'puerto rican', 'last night', 'little boy', 'taco bell', 'statue liberty', 'bus driver', 'ice cream', 'don know', 'high school']\n",
      "pirates.txt ['jack sparrow', 'will turner', 'elizabeth swann', 'davy jones', 'flying dutchman', 'lord cutler', 'cutler beckett', 'black pearl', 'tia dalma', 'heh heh', 'edinburgh trader', 'port royal', 'bamboo pole', 'east india', 'jar dirt']\n",
      "singles.txt ['non smoker', 'would like', 'dining out', 'like meet', 'age open', 'sense humour', 'looking for', 'social drinker', 'long term', 'down earth', 'quiet nights', 'easy going', 'medium build', 'nights home', 'weekends away']\n",
      "wine.txt ['high toned', 'top ***', 'not rated', 'few years', 'medium weight', 'year two', 'cigar box', 'cote rotie', 'mixed feelings', 'demi sec', 'from half', 'brown sugar', 'bare ****', 'tightly wound', 'lynch bages']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import webtext\n",
    "\n",
    "for file in webtext.fileids():\n",
    "    words = [word.lower() for word in webtext.words(file) if len(word) > 2]\n",
    "    print (file, [w1+' '+w2 for w1, w2 in collocations(words)[:15]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Rewrite `collocations()` that takes a `threshold` parameter to filter out collocations whose score doesn't meet the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "threshold: 0  collocations: 113264\n",
      "threshold: 1  collocations: 585\n",
      "threshold: 2  collocations: 48\n",
      "threshold: 3  collocations: 26\n",
      "threshold: 4  collocations: 9\n",
      "threshold: 5  collocations: 6\n",
      "threshold: 6  collocations: 4\n",
      "threshold: 7  collocations: 3\n",
      "threshold: 8  collocations: 3\n",
      "threshold: 9  collocations: 3\n"
     ]
    }
   ],
   "source": [
    "moby_dick = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
    "words = [word.lower() for word in moby_dick if len(word) > 2]\n",
    "for thresh in range(10):\n",
    "    collocs = collocations2(words, thresh)\n",
    "    print(f'threshold: {thresh}  collocations: {len(collocs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collocations2(words, threshold):\n",
    "    # Count the words and bigrams\n",
    "    wfd = nltk.FreqDist(words)\n",
    "    pfd = nltk.FreqDist(bigrams(words))\n",
    "\n",
    "    scored = [((w1,w2), score(w1, w2, wfd, pfd)) for w1, w2 in pfd]\n",
    "    ## sort according to the score\n",
    "    scored = [pair for pair in scored if first(pair) >= threshold]\n",
    "    scored.sort(key=first, reverse=True)\n",
    "    return [p for (p,s) in scored]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tags\n",
    "\n",
    "Part-of-speech tags, also called \"word categories\", are a shallow annotation on top of words that give a hint as to their syntactic function. It is not the case that all languages have the same parts of speech, and some linguists even reject the notion altogether, but for computational linguists they are often useful for modeling language.\n",
    "\n",
    "When serialized, POS tags are conventionally written following the word they annotate, separated with a slash:\n",
    "\n",
    "```\n",
    "The/DT artist/NN will/RB record/VBZ a/DT new/JJ record/NN ./.\n",
    "```\n",
    "\n",
    "In Python, these may be loaded as a list of pairs:\n",
    "\n",
    "```python\n",
    "  [('The', 'DT'), ('artist', 'NN'), ('will', 'RB'), ('record', 'VBZ'),\n",
    "   ('a', 'DT'), ('new', 'JJ'), ('record', 'NN'), ('.', '.')]\n",
    "```\n",
    "\n",
    "The NLTK has several functions defined for tagging text and working with POS-tagged data. If you have a list of words, you can use a basic tagger with `nltk.pos_tag()`. You might also want to use `nltk.word_tokenize()` to break a sentence into a list of words as NLTK expects. Try them out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/goodmami/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('artist', 'NN'),\n",
       " ('will', 'MD'),\n",
       " ('record', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('new', 'JJ'),\n",
       " ('record', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import nltk.tag\n",
    "tagged = 'The/DT artist/NN will/RB record/VBZ a/DT new/JJ record/NN ./.'\n",
    "words = 'The artist will record a new record.' # tokenize a sentence\n",
    "tokens = nltk.word_tokenize(words)\n",
    "tagged_pairs = nltk.pos_tag(tokens)\n",
    "tagged_pairs\n",
    "# pos-tag the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have tagged text (in the `word/NN` serialization), you can use `nltk.tag.str2tuple()`. Try it out (you can use the example above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('artist', 'NN'),\n",
       " ('will', 'RB'),\n",
       " ('record', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('new', 'JJ'),\n",
       " ('record', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "# convert a word/TAG pair to a tuple\n",
    "pairs = [nltk.str2tuple(tok) for tok in tagged.split()]\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with multilingual data, it would probably help to ensure the data from different languages uses the same set of tags. The `nltk.map_tag()` function can map from a known tag set to another, and the 'universal' tags are generalized to be relevant for many languages.\n",
    "\n",
    "```\n",
    ">>> nltk.map_tag('en-ptb', 'universal', 'NN')\n",
    "'NOUN'\n",
    "```\n",
    "\n",
    "**Task:** Try to convert the tags for the sentence above into universal tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The DET\nartist NOUN\nwill VERB\nrecord VERB\na DET\nnew ADJ\nrecord NOUN\n. .\n"
     ]
    }
   ],
   "source": [
    "for word, tag in tagged_pairs:\n",
    "    utag = nltk.map_tag('en-ptb', 'universal', tag)\n",
    "    print(word, utag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n[nltk_data]     /home/goodmami/nltk_data...\n[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('artist', 'NOUN'),\n",
       " ('will', 'VERB'),\n",
       " ('record', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('new', 'ADJ'),\n",
       " ('record', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "nltk.download('universal_tagset')\n",
    "[(word, nltk.map_tag('en-ptb', 'universal', tag)) for word, tag in tagged_pairs]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}