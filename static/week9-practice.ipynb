{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 09\n",
    "\n",
    "Overview\n",
    "\n",
    "* [**First-class Functions**](#First-class-Functions)\n",
    "* [**Higher-order Functions**](#Higher-order-Functions)\n",
    "* [**Recursive Functions**](#Recursive-Functions)\n",
    "* [**Bigrams**](#Bigrams)\n",
    "* [**N-grams**](#N-grams)\n",
    "* [**Collocations**](#Collocations)\n",
    "* [**Part-of-Speech Tags**](#Part-of-Speech-Tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "source": [
    "## First-class Functions\n",
    "\n",
    "Functions in Python are just a special kind of object, and you can use them in many ways that you can use other kinds of objects, like integers, strings, etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(x):\n",
    "    print(f'x={x}')\n",
    "\n",
    "func = function   # reassignment\n",
    "func('123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func == function  # comparison"
   ]
  },
  {
   "source": [
    "This flexibility enables higher-order functions.\n",
    "\n",
    "## Higher-order Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Use `filter()` with `str.isdigit()` to filter out non-numbers from a list of strings."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = 'one 2 three 4 five 6 seven 8'.split()\n",
    "number_strings = filter()  # TODO"
   ]
  },
  {
   "source": [
    "Now use `map()` with `int` to convert them all to integers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = map()  # TODO"
   ]
  },
  {
   "source": [
    "Write a higher-order function that takes a text-normalization function (such as `str.lower()`) and returns a new function that takes a list of strings and applies the normalization function to each one, returning a new list of strings."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_normalizer(func):\n",
    "    # TODO: define new function; don't forget to return it!\n",
    "\n",
    "all_upper = make_normalizer(str.upper)\n",
    "print(all_upper(\"don't panic\".split()))"
   ]
  },
  {
   "source": [
    "## Recursive Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Write a recursive function to compute the fibonacci sequence, defined as:\n",
    "\n",
    "$$ fib(0) = 1 $$\n",
    "$$ fib(1) = 1 $$\n",
    "$$ fib(x) = fib(x-2) + fib(x-1) $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib(n):\n",
    "    # TODO: write recursive fib()"
   ]
  },
  {
   "source": [
    "Try it out for a few values, like 3, 5, 10, 20, 40, ...\n",
    "\n",
    "Now try to write it iteratively."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterfib(n):\n",
    "    # TODO: write iterative fibonacci function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams\n",
    "\n",
    "Bigrams are sequential pairs of items in a list. With language data, these items are generally words or characters. The bigrams (and n-grams in general) are like a sliding window of a small sequence of the data, and capture partial order information (e.g., \"the\" precedes \"dog\" in *the dog barked*). The bigrams for the sentence *Bigrams and n-grams are useful tools for computational linguists.* are as follows (with list indices indicated):\n",
    "\n",
    "```\n",
    "      Bigrams and n-grams are useful tools for computational linguists .\n",
    "     0       1   2       3   4      5     6   7             8         9 10\n",
    "0:2   Bigrams and\n",
    "1:3           and n-grams\n",
    "2:4               n-grams are\n",
    "3:5                       are useful\n",
    "4:6                           useful tools\n",
    "5:7                                  tools for\n",
    "6:8                                        for computational\n",
    "7:9                                            computational linguists\n",
    "8:10                                                         linguists .\n",
    "```\n",
    "\n",
    "**Task:** write a function to take an sequence and return a list of its bigrams. Compare your results with `nltk.bigrams()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def bigrams(seq):\n",
    "    \"\"\"\n",
    "    Return the list of bigrams for *seq*.\n",
    "    \n",
    "    Each bigram on the list should be a tuple.\n",
    "    \"\"\"\n",
    "    return []  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = 'Bigrams and n-grams are useful tools for computational linguists .'.split()\n",
    "print('Test:', list(bigrams(words)))\n",
    "print()\n",
    "print('NLTK:', list(nltk.bigrams(words)))\n",
    "print()\n",
    "print('Equal?', list(bigrams(words)) == list(nltk.bigrams(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "Sometimes bigrams do not give enough context (they are not a big enough window into the sequence). N-grams are the general form of bigrams, where N=2. Write a function that takes a sequence and a number `n` and returns the list of n-grams for the sequence. Note that if `n` is greater than the length of the list, an empty list is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(seq, n):\n",
    "    \"\"\"\n",
    "    Return the list of n-grams for *seq* of length *n*.\n",
    "    \n",
    "    Each n-gram should be a tuple. If *n* is greater than the length\n",
    "    of *seq*, an empty list is returned. *n* must be greater than 0.\n",
    "    \"\"\"\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = 'Bigrams and n-grams are useful tools for computational linguists .'.split()\n",
    "print('1-grams:', ist(ngrams(words, 1)))\n",
    "print()\n",
    "print('2-grams:', list(ngrams(words, 2)))\n",
    "print()\n",
    "print('3-grams:', list(ngrams(words, 3)))\n",
    "print()\n",
    "print('20-grams:', list(ngrams(words, 20)))\n",
    "print()\n",
    "print('Equal?', list(ngrams(words, 3)) == list(nltk.ngrams(words, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations\n",
    "\n",
    "Note that some n-grams, such as `('are', 'useful')` are probably more frequent than others, such as `('Bigrams', 'and')`. N-grams whose components co-occur frequently are called \"collocations\". With an appropriate metric you can compute a collocation score for these, which could also be used as a threshold. Mutual information (MI) is a measure using the *observed frequencies* (O) and *expected frequencies* (E):\n",
    "\n",
    "$$ MI = log \\frac{O}{E} $$\n",
    "\n",
    "The observed frequency is how often we see the n-gram in the data. The expected frequency is how often we expect to see it given the frequencies of its components. Here is one way to define collocations (adapted from the reading):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# itemgetter() is a function that takes a number n and returns a\n",
    "# function which takes a sequence and returns the item at index n\n",
    "first = itemgetter(1)\n",
    "\n",
    "def collocations(words):\n",
    "    # Count the words and bigrams\n",
    "    wfd = nltk.FreqDist(words)\n",
    "    pfd = nltk.FreqDist(bigrams(words))\n",
    "\n",
    "    scored = [((w1,w2), score(w1, w2, wfd, pfd)) for w1, w2 in pfd]\n",
    "    ## sort according to the score\n",
    "    scored.sort(key=first, reverse=True)\n",
    "    return [p for (p,s) in scored]\n",
    "\n",
    "\n",
    "def score(word1, word2, wfd, pfd, power=3):\n",
    "    '''return the collocation score f(w1,w2)^power/(f(w1)*f(w2))'''\n",
    "    freq1 = wfd[word1]\n",
    "    freq2 = wfd[word2]\n",
    "    freq12 = pfd[(word1, word2)]\n",
    "    return freq12 ** power / float(freq1 * freq2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext\n",
    "\n",
    "for file in webtext.fileids():\n",
    "    words = [word.lower() for word in webtext.words(file) if len(word) > 2]\n",
    "    print (file, [w1+' '+w2 for w1, w2 in collocations(words)[:15]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Rewrite `collocations()` that takes a `threshold` parameter to filter out collocations whose score doesn't meet the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collocations2(words, threshold):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tags\n",
    "\n",
    "Part-of-speech tags, also called \"word categories\", are a shallow annotation on top of words that give a hint as to their syntactic function. It is not the case that all languages have the same parts of speech, and some linguists even reject the notion altogether, but for computational linguists they are often useful for modeling language.\n",
    "\n",
    "When serialized, POS tags are conventionally written following the word they annotate, separated with a slash:\n",
    "\n",
    "```\n",
    "The/DT artist/NN will/RB record/VBZ a/DT new/JJ record/NN ./.\n",
    "```\n",
    "\n",
    "In Python, these may be loaded as a list of pairs:\n",
    "\n",
    "```python\n",
    "  [('The', 'DT'), ('artist', 'NN'), ('will', 'RB'), ('record', 'VBZ'),\n",
    "   ('a', 'DT'), ('new', 'JJ'), ('record', 'NN'), ('.', '.')]\n",
    "```\n",
    "\n",
    "The NLTK has several functions defined for tagging text and working with POS-tagged data. If you have a list of words, you can use a basic tagger with `nltk.pos_tag()`. You might also want to use `nltk.word_tokenize()` to break a sentence into a list of words as NLTK expects. Try them out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.tag\n",
    "words = # tokenize a sentence\n",
    "# pos-tag the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have tagged text (in the `word/NN` serialization), you can use `nltk.tag.str2tuple()`. Try it out (you can use the example above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a word/TAG pair to a tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with multilingual data, it would probably help to ensure the data from different languages uses the same set of tags. The `nltk.map_tag()` function can map from a known tag set to another, and the 'universal' tags are generalized to be relevant for many languages.\n",
    "\n",
    "```\n",
    ">>> nltk.map_tag('en-ptb', 'universal', 'NN')\n",
    "'NOUN'\n",
    "```\n",
    "\n",
    "**Task:** Try to convert the tags for the sentence above into universal tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}